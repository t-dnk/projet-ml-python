{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f881005-5e6d-42cd-befc-953c5fe27d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET OVERVIEW\n",
      "================================================================================\n",
      "\n",
      "Dataset shape: 1338 rows, 1 columns\n",
      "\n",
      "\n",
      "First rows:\n",
      "  age;sex;bmi;children;smoker;region;charges\n",
      "0   19;female;27.9;0;yes;southwest;16884.924\n",
      "1     18;male;33.77;1;no;southeast;1725.5523\n",
      "2         28;male;33;3;no;southeast;4449.462\n",
      "3  33;male;22.705;0;no;northwest;21984.47061\n",
      "4     32;male;28.88;0;no;northwest;3866.8552\n",
      "\n",
      "\n",
      "Dataset information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1338 entries, 0 to 1337\n",
      "Data columns (total 1 columns):\n",
      " #   Column                                      Non-Null Count  Dtype \n",
      "---  ------                                      --------------  ----- \n",
      " 0   age;sex;bmi;children;smoker;region;charges  1338 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 10.6+ KB\n",
      "None\n",
      "\n",
      "\n",
      "Variables definition:\n",
      "\n",
      "- age: Age of the policyholder (integer)\n",
      "- sex: Gender of the policyholder (categorical: male/female)\n",
      "- bmi: Body Mass Index (float)\n",
      "- children: Number of children/dependents (integer)\n",
      "- smoker: Smoking status (categorical: yes/no)\n",
      "- region: Residential area (categorical: northeast/northwest/southeast/southwest)\n",
      "- charges: Individual medical costs billed by insurance (float) [TARGET VARIABLE]\n",
      "\n",
      "================================================================================\n",
      "DATA QUALITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Missing values per column:\n",
      "age;sex;bmi;children;smoker;region;charges    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate rows: 1\n",
      "Duplicates removed. New shape: (1337, 1)\n",
      "\n",
      "================================================================================\n",
      "DESCRIPTIVE STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Numerical variables summary:\n",
      "        age;sex;bmi;children;smoker;region;charges\n",
      "count                                         1337\n",
      "unique                                        1337\n",
      "top     61;female;29.07;0;yes;northwest;29141.3603\n",
      "freq                                             1\n",
      "\n",
      "\n",
      "Categorical variables distribution:\n",
      "\n",
      "SEX:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sex'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sex'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 83\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmoker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregion\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df[col]\u001b[38;5;241m.\u001b[39mvalue_counts())\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProportion:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdf[col]\u001b[38;5;241m.\u001b[39mvalue_counts(normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# 5. DATA VISUALIZATION\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sex'"
     ]
    }
   ],
   "source": [
    "# INSURANCE COST PREDICTION - COMPLETE MACHINE LEARNING PROJECT\n",
    "\n",
    "# =============================================================================\n",
    "# 1. LIBRARIES IMPORTATION\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, learning_curve\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATASET LOADING AND INITIAL EXPLORATION\n",
    "# =============================================================================\n",
    "\n",
    "df = pd.read_csv('insurance.csv', sep=',')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset shape: {df.shape[0]} rows, {df.shape[1]} columns\\n\")\n",
    "\n",
    "print(\"\\nFirst rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n\\nDataset information:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n\\nVariables definition:\")\n",
    "print(\"\"\"\n",
    "- age: Age of the policyholder (integer)\n",
    "- sex: Gender of the policyholder (categorical: male/female)\n",
    "- bmi: Body Mass Index (float)\n",
    "- children: Number of children/dependents (integer)\n",
    "- smoker: Smoking status (categorical: yes/no)\n",
    "- region: Residential area (categorical: northeast/northwest/southeast/southwest)\n",
    "- charges: Individual medical costs billed by insurance (float) [TARGET VARIABLE]\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. DATA QUALITY ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicates if any\n",
    "if df.duplicated().sum() > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Duplicates removed. New shape: {df.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. DESCRIPTIVE STATISTICS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nNumerical variables summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\n\\nCategorical variables distribution:\")\n",
    "for col in ['sex', 'smoker', 'region']:\n",
    "    print(f\"\\n{col.upper()}:\")\n",
    "    print(df[col].value_counts())\n",
    "    print(f\"Proportion:\\n{df[col].value_counts(normalize=True) * 100}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. DATA VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Distribution of numerical variables\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "numerical_cols = ['age', 'bmi', 'children', 'charges']\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    ax = axes[i//3, i%3]\n",
    "    df[col].hist(bins=30, edgecolor='black', ax=ax, color='skyblue')\n",
    "    ax.set_title(f'Distribution of {col}')\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Categorical variables visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "sns.countplot(data=df, x='smoker', palette='Set2', ax=axes[0])\n",
    "axes[0].set_title('Smoker Distribution')\n",
    "\n",
    "sns.countplot(data=df, x='sex', palette='pastel', ax=axes[1])\n",
    "axes[1].set_title('Gender Distribution')\n",
    "\n",
    "sns.countplot(data=df, x='region', palette='viridis', ax=axes[2])\n",
    "axes[2].set_title('Region Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('categorical_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Charges analysis by categorical variables\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "sns.boxplot(data=df, x='smoker', y='charges', palette='coolwarm', ax=axes[0])\n",
    "axes[0].set_title('Charges by Smoker Status')\n",
    "\n",
    "sns.boxplot(data=df, x='sex', y='charges', palette='Set1', ax=axes[1])\n",
    "axes[1].set_title('Charges by Gender')\n",
    "\n",
    "sns.boxplot(data=df, x='region', y='charges', palette='Set3', ax=axes[2])\n",
    "axes[2].set_title('Charges by Region')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('charges_by_category.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Correlation analysis\n",
    "plt.figure(figsize=(10, 8))\n",
    "df_encoded = df.copy()\n",
    "df_encoded['sex'] = df_encoded['sex'].map({'female': 0, 'male': 1})\n",
    "df_encoded['smoker'] = df_encoded['smoker'].map({'no': 0, 'yes': 1})\n",
    "correlation_matrix = df_encoded[['age', 'sex', 'bmi', 'children', 'smoker', 'charges']].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, square=True, linewidths=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey correlations with charges:\")\n",
    "print(correlation_matrix['charges'].sort_values(ascending=False))\n",
    "\n",
    "# Scatter plots for important relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "sns.scatterplot(data=df, x='age', y='charges', hue='smoker', palette='Set1', ax=axes[0,0], alpha=0.6)\n",
    "axes[0,0].set_title('Age vs Charges (colored by Smoker)')\n",
    "\n",
    "sns.scatterplot(data=df, x='bmi', y='charges', hue='smoker', palette='Set1', ax=axes[0,1], alpha=0.6)\n",
    "axes[0,1].set_title('BMI vs Charges (colored by Smoker)')\n",
    "\n",
    "sns.scatterplot(data=df, x='age', y='bmi', hue='smoker', palette='Set1', ax=axes[1,0], alpha=0.6)\n",
    "axes[1,0].set_title('Age vs BMI (colored by Smoker)')\n",
    "\n",
    "sns.violinplot(data=df, x='children', y='charges', palette='muted', ax=axes[1,1])\n",
    "axes[1,1].set_title('Charges by Number of Children')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('scatter_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 6. OUTLIERS DETECTION AND TREATMENT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OUTLIERS ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Check outliers for numerical columns\n",
    "numerical_cols = ['age', 'bmi', 'children', 'charges']\n",
    "outliers_summary = {}\n",
    "\n",
    "for col in numerical_cols:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df, col)\n",
    "    outliers_summary[col] = len(outliers)\n",
    "    print(f\"\\n{col.upper()}:\")\n",
    "    print(f\"  Outliers detected: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "    print(f\"  Range: [{lower:.2f}, {upper:.2f}]\")\n",
    "\n",
    "# Visualize outliers\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    axes[i].boxplot(df[col], vert=True)\n",
    "    axes[i].set_title(f'{col} - Outliers')\n",
    "    axes[i].set_ylabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outliers_boxplots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDecision: Keep outliers as they represent valid high-cost insurance cases\")\n",
    "\n",
    "\n",
    "\n",
    "# 7. DATA PREPROCESSING\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Encodage des variables catégorielles\n",
    "df['sex'] = df['sex'].map({'female': 0, 'male': 1})\n",
    "df['smoker'] = df['smoker'].map({'no': 0, 'yes': 1})\n",
    "df = pd.get_dummies(df, columns=['region'], drop_first=True)\n",
    "\n",
    "# Séparation des variables explicatives et de la cible\n",
    "X = df.drop('charges', axis=1)\n",
    "y = df['charges']\n",
    "\n",
    "# Mise à l’échelle des variables\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Découpage en train et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"Preprocessing completed successfully\")\n",
    "\n",
    "# BASELINE : Linear Regression\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BASELINE MODEL - Linear Regression\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_pred_lin = lin_reg.predict(X_test)\n",
    "\n",
    "print(\"\\nLinear Regression:\")\n",
    "print(\"  MAE :\", round(mean_absolute_error(y_test, y_pred_lin), 2))\n",
    "print(\"  RMSE:\", round(np.sqrt(mean_squared_error(y_test, y_pred_lin)), 2))\n",
    "print(\"  R²  :\", round(r2_score(y_test, y_pred_lin), 4))\n",
    "\n",
    "# REGULARIZED MODELS : Ridge & Lasso\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"REGULARIZED LINEAR MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge = Ridge(alpha=1.0, random_state=42)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "print(\"\\nRidge Regression:\")\n",
    "print(\"  MAE :\", round(mean_absolute_error(y_test, y_pred_ridge), 2))\n",
    "print(\"  RMSE:\", round(np.sqrt(mean_squared_error(y_test, y_pred_ridge)), 2))\n",
    "print(\"  R²  :\", round(r2_score(y_test, y_pred_ridge), 4))\n",
    "\n",
    "# Lasso Regression\n",
    "lasso = Lasso(alpha=1.0, random_state=42)\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "print(\"\\nLasso Regression:\")\n",
    "print(\"  MAE :\", round(mean_absolute_error(y_test, y_pred_lasso), 2))\n",
    "print(\"  RMSE:\", round(np.sqrt(mean_squared_error(y_test, y_pred_lasso)), 2))\n",
    "print(\"  R²  :\", round(r2_score(y_test, y_pred_lasso), 4))\n",
    "\n",
    "# TREE-BASED MODELS\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TREE-BASED MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "print(\"\\nDecision Tree:\")\n",
    "print(\"  MAE :\", round(mean_absolute_error(y_test, y_pred_dt), 2))\n",
    "print(\"  RMSE:\", round(np.sqrt(mean_squared_error(y_test, y_pred_dt)), 2))\n",
    "print(\"  R²  :\", round(r2_score(y_test, y_pred_dt), 4))\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"\\nRandom Forest:\")\n",
    "print(\"  MAE :\", round(mean_absolute_error(y_test, y_pred_rf), 2))\n",
    "print(\"  RMSE:\", round(np.sqrt(mean_squared_error(y_test, y_pred_rf)), 2))\n",
    "print(\"  R²  :\", round(r2_score(y_test, y_pred_rf), 4))\n",
    "\n",
    "\n",
    "# TREE-BASED MODELS\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "# BASELINE : Linear Regression\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BASELINE MODEL - Linear Regression\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_lin = lin_reg.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nLinear Regression:\")\n",
    "print(\"  MAE :\", round(mean_absolute_error(y_test, y_pred_lin), 2))\n",
    "print(\"  RMSE:\", round(np.sqrt(mean_squared_error(y_test, y_pred_lin)), 2))\n",
    "print(\"  R²  :\", round(r2_score(y_test, y_pred_lin), 4))\n",
    "\n",
    "\n",
    "# REGULARIZED MODEL : Ridge\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"REGULARIZED MODEL - Ridge Regression\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "ridge = Ridge(alpha=1.0, random_state=42)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "y_pred_ridge = ridge.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nRidge Regression:\")\n",
    "print(\"  MAE :\", round(mean_absolute_error(y_test, y_pred_ridge), 2))\n",
    "print(\"  RMSE:\", round(np.sqrt(mean_squared_error(y_test, y_pred_ridge)), 2))\n",
    "print(\"  R²  :\", round(r2_score(y_test, y_pred_ridge), 4))\n",
    "\n",
    "\n",
    "# TREE-BASED MODEL : Decision Tree\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TREE-BASED MODEL - Decision Tree\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "dt.fit(X_train_scaled, y_train)\n",
    "y_pred_dt = dt.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nDecision Tree:\")\n",
    "print(\"  MAE :\", round(mean_absolute_error(y_test, y_pred_dt), 2))\n",
    "print(\"  RMSE:\", round(np.sqrt(mean_squared_error(y_test, y_pred_dt)), 2))\n",
    "print(\"  R²  :\", round(r2_score(y_test, y_pred_dt), 4))\n",
    "\n",
    "\n",
    "\n",
    "# ENSEMBLE MODEL : Random Forest\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ENSEMBLE MODEL - Random Forest\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = rf.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nRandom Forest:\")\n",
    "print(\"  MAE :\", round(mean_absolute_error(y_test, y_pred_rf), 2))\n",
    "print(\"  RMSE:\", round(np.sqrt(mean_squared_error(y_test, y_pred_rf)), 2))\n",
    "print(\"  R²  :\", round(r2_score(y_test, y_pred_rf), 4))\n",
    "\n",
    "\n",
    "# LEARNING CURVES ANALYSIS\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LEARNING CURVES ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Courbes d'apprentissage = évolution des performances quand on augmente la taille du jeu d'entraînement\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    lin_reg, X_train_scaled, y_train,\n",
    "    cv=5, scoring='neg_mean_absolute_error',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5), n_jobs=-1\n",
    ")\n",
    "\n",
    "# Moyenne des scores\n",
    "train_mean = -train_scores.mean(axis=1)\n",
    "val_mean = -val_scores.mean(axis=1)\n",
    "\n",
    "# Tracé des courbes\n",
    "plt.plot(train_sizes, train_mean, label='Training MAE', color='blue', marker='o')\n",
    "plt.plot(train_sizes, val_mean, label='Validation MAE', color='red', marker='o')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Learning Curve - Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Résultats finaux\n",
    "print(\"\\nLinear Regression:\")\n",
    "print(\"  Training MAE :\", round(train_mean[-1], 2))\n",
    "print(\"  Validation MAE :\", round(val_mean[-1], 2))\n",
    "print(\"  Gap :\", round(val_mean[-1] - train_mean[-1], 2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 13. FEATURE IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Random Forest feature importance\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': best_rf.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nRANDOM FOREST - Feature Importance:\")\n",
    "print(feature_importance_rf)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance_rf, x='Importance', y='Feature', palette='viridis')\n",
    "plt.title('Feature Importance - Random Forest')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance_rf.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Gradient Boosting feature importance\n",
    "feature_importance_gb = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': best_gb.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nGRADIENT BOOSTING - Feature Importance:\")\n",
    "print(feature_importance_gb)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance_gb, x='Importance', y='Feature', palette='plasma')\n",
    "plt.title('Feature Importance - Gradient Boosting')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance_gb.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 14. DIMENSIONALITY REDUCTION - PCA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DIMENSIONALITY REDUCTION - PCA ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print(\"\\nExplained variance by component:\")\n",
    "for i, var in enumerate(explained_variance):\n",
    "    print(f\"  PC{i+1}: {var:.4f} ({cumulative_variance[i]:.4f} cumulative)\")\n",
    "\n",
    "# Plot explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, color='steelblue')\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('PCA - Explained Variance by Component')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='-', color='darkred')\n",
    "axes[1].axhline(y=0.95, color='green', linestyle='--', label='95% threshold')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title('PCA - Cumulative Explained Variance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pca_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Find number of components for 95% variance\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"\\nNumber of components to explain 95% variance: {n_components_95}\")\n",
    "\n",
    "# Train model with reduced dimensions\n",
    "if n_components_95 < X_train_scaled.shape[1]:\n",
    "    pca_reduced = PCA(n_components=n_components_95)\n",
    "    X_train_pca_reduced = pca_reduced.fit_transform(X_train_scaled)\n",
    "    X_test_pca_reduced = pca_reduced.transform(X_test_scaled)\n",
    "    \n",
    "    rf_pca = RandomForestRegressor(**grid_rf.best_params_, random_state=42)\n",
    "    rf_pca.fit(X_train_pca_reduced, y_train)\n",
    "    y_pred_rf_pca = rf_pca.predict(X_test_pca_reduced)\n",
    "    \n",
    "    mae_rf_pca = mean_absolute_error(y_test, y_pred_rf_pca)\n",
    "    r2_rf_pca = r2_score(y_test, y_pred_rf_pca)\n",
    "    \n",
    "    print(f\"\\nRandom Forest with PCA ({n_components_95} components):\")\n",
    "    print(f\"  MAE: {mae_rf_pca:.2f}\")\n",
    "    print(f\"  R²: {r2_rf_pca:.4f}\")\n",
    "    print(f\"  Performance change: {((mae_rf - mae_rf_pca) / mae_rf * 100):.2f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# 15. RESIDUALS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESIDUALS ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate residuals for best model (Gradient Boosting)\n",
    "residuals = y_test - y_pred_gb\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Residuals vs Predicted\n",
    "axes[0, 0].scatter(y_pred_gb, residuals, alpha=0.6, edgecolors='k')\n",
    "axes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Predicted Values')\n",
    "axes[0, 0].set_ylabel('Residuals')\n",
    "axes[0, 0].set_title('Residuals vs Predicted Values')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram of residuals\n",
    "axes[0, 1].hist(residuals, bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "axes[0, 1].set_xlabel('Residuals')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Residuals')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[1, 1].scatter(y_test, y_pred_gb, alpha=0.6, edgecolors='k')\n",
    "axes[1, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Actual Values')\n",
    "axes[1, 1].set_ylabel('Predicted Values')\n",
    "axes[1, 1].set_title('Actual vs Predicted Values')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('residuals_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Residuals statistics\n",
    "print(\"\\nResiduals Statistics:\")\n",
    "print(f\"  Mean: {residuals.mean():.2f}\")\n",
    "print(f\"  Std Dev: {residuals.std():.2f}\")\n",
    "print(f\"  Min: {residuals.min():.2f}\")\n",
    "print(f\"  Max: {residuals.max():.2f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 16. FINAL MODEL COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Linear Regression',\n",
    "        'Ridge Regression',\n",
    "        'Lasso Regression',\n",
    "        'ElasticNet',\n",
    "        'Decision Tree',\n",
    "        'Random Forest (tuned)',\n",
    "        'Gradient Boosting (tuned)',\n",
    "        'Voting Ensemble',\n",
    "        'Stacking Ensemble'\n",
    "    ],\n",
    "    'MAE': [mae_test_lr, mae_ridge, mae_lasso, mae_elasticnet, mae_dt, \n",
    "            mae_rf, mae_gb, mae_voting, mae_stacking],\n",
    "    'RMSE': [rmse_test_lr, rmse_ridge, rmse_lasso, rmse_elasticnet, rmse_dt,\n",
    "             rmse_rf, rmse_gb, rmse_voting, rmse_stacking],\n",
    "    'R²': [r2_test_lr, r2_ridge, r2_lasso, r2_elasticnet, r2_dt,\n",
    "           r2_rf, r2_gb, r2_voting, r2_stacking]\n",
    "})\n",
    "\n",
    "results = results.sort_values(by='MAE', ascending=True)\n",
    "print(\"\\n\", results.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "sns.barplot(data=results, x='MAE', y='Model', palette='viridis', ax=axes[0])\n",
    "axes[0].set_title('Mean Absolute Error Comparison')\n",
    "axes[0].set_xlabel('MAE')\n",
    "\n",
    "sns.barplot(data=results, x='RMSE', y='Model', palette='plasma', ax=axes[1])\n",
    "axes[1].set_title('Root Mean Squared Error Comparison')\n",
    "axes[1].set_xlabel('RMSE')\n",
    "\n",
    "sns.barplot(data=results, x='R²', y='Model', palette='coolwarm', ax=axes[2])\n",
    "axes[2].set_title('R² Score Comparison')\n",
    "axes[2].set_xlabel('R² Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 17. BEST MODEL SELECTION AND FINAL EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BEST MODEL SELECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_model_name = results.iloc[0]['Model']\n",
    "best_mae = results.iloc[0]['MAE']\n",
    "best_rmse = results.iloc[0]['RMSE']\n",
    "best_r2 = results.iloc[0]['R²']\n",
    "\n",
    "print(f\"\\nBest performing model: {best_model_name}\")\n",
    "print(f\"  MAE: {best_mae:.2f}\")\n",
    "print(f\"  RMSE: {best_rmse:.2f}\")\n",
    "print(f\"  R²: {best_r2:.4f}\")\n",
    "\n",
    "# Calculate MAPE for best model\n",
    "if best_model_name == 'Gradient Boosting (tuned)':\n",
    "    best_model = best_gb\n",
    "    y_pred_best = y_pred_gb\n",
    "elif best_model_name == 'Random Forest (tuned)':\n",
    "    best_model = best_rf\n",
    "    y_pred_best = y_pred_rf\n",
    "elif best_model_name == 'Stacking Ensemble':\n",
    "    best_model = stacking\n",
    "    y_pred_best = y_pred_stacking\n",
    "else:\n",
    "    best_model = voting\n",
    "    y_pred_best = y_pred_voting\n",
    "\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred_best)\n",
    "print(f\"  MAPE: {mape*100:.2f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# 18. CONCLUSIONS AND RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONCLUSIONS AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "KEY FINDINGS:\n",
    "\n",
    "1. DATA INSIGHTS:\n",
    "   - Smoking status is the strongest predictor of insurance charges\n",
    "   - Strong correlation between age and charges, especially for smokers\n",
    "   - BMI shows moderate correlation with charges\n",
    "   - Gender and region have minimal impact on charges\n",
    "\n",
    "2. MODEL PERFORMANCE:\n",
    "   - Tree-based ensemble methods (RF, GB) significantly outperform linear models\n",
    "   - Gradient Boosting achieved the best balance of accuracy and generalization\n",
    "   - Ensemble methods (Voting/Stacking) provide stable predictions\n",
    "   - Linear models show underfitting, while complex trees show slight overfitting\n",
    "\n",
    "3. FEATURE IMPORTANCE:\n",
    "   - Top predictors: smoker status, age, bmi\n",
    "   - Number of children and region have lower importance\n",
    "   - PCA analysis shows most variance captured by few components\n",
    "\n",
    "4. RECOMMENDATIONS:\n",
    "   - Use Gradient Boosting or ensemble models for production\n",
    "   - Focus data collection on key features: smoking status, age, BMI\n",
    "   - Consider interaction features (age × smoker, BMI × smoker)\n",
    "   - Monitor model performance regularly and retrain with new data\n",
    "\n",
    "REFERENCES:\n",
    "- Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5-32.\n",
    "- Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine.\n",
    "- Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning.\n",
    "- Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System.\n",
    "\n",
    "PROJECT COMPLETED SUCCESSFULLY\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed59d0-2a30-40d7-9e3e-3c1034b32983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1960868-5a13-4582-9b0d-7b7d8f7836bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
