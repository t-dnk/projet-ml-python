# =============================================================================
# 7. DATA PREPROCESSING
# =============================================================================

print("\n" + "=" * 80)
print("DATA PREPROCESSING")
print("=" * 80)

# Préparation des données
df_processed = df.copy()
df_processed['sex'] = df_processed['sex'].map({'female': 0, 'male': 1})
df_processed['smoker'] = df_processed['smoker'].map({'no': 0, 'yes': 1})
df_processed = pd.get_dummies(df_processed, columns=['region'], drop_first=True)

# Séparation features/target et train/test
X = df_processed.drop('charges', axis=1)
y = df_processed['charges']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalisation
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"Train: {X_train.shape[0]} samples | Test: {X_test.shape[0]} samples")

# =============================================================================
# 8- ENTRAÎNEMENT DES MODÈLES
# =============================================================================

print("\n" + "=" * 80)
print("MODÈLES ET ÉVALUATION")
print("=" * 80)

def evaluate_model(name, y_true, y_pred):
    """Évalue et affiche les métriques d'un modèle"""
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    
    print(f"\n{name}:")
    print(f"  MAE: {mae:.2f} | RMSE: {rmse:.2f} | R²: {r2:.4f}")
    return mae, rmse, r2

# 1. Linear Regression
lr = LinearRegression()
lr.fit(X_train_scaled, y_train)
evaluate_model("LINEAR REGRESSION", y_test, lr.predict(X_test_scaled))

# 2. Decision Tree
dt = DecisionTreeRegressor(max_depth=10, random_state=42)
dt.fit(X_train_scaled, y_train)
evaluate_model("DECISION TREE", y_test, dt.predict(X_test_scaled))

# 3. Random Forest avec tuning
print("\nRANDOM FOREST - Tuning en cours...")
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
rf = RandomForestRegressor(random_state=42)
grid_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=1)
grid_rf.fit(X_train_scaled, y_train)
best_rf = grid_rf.best_estimator_
print(f"Best params: {grid_rf.best_params_}")
evaluate_model("RANDOM FOREST", y_test, best_rf.predict(X_test_scaled))

# 4. Gradient Boosting avec tuning
print("\nGRADIENT BOOSTING - Tuning en cours...")
param_grid_gb = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2]
}
gb = GradientBoostingRegressor(random_state=42)
grid_gb = GridSearchCV(gb, param_grid_gb, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=1)
grid_gb.fit(X_train_scaled, y_train)
best_gb = grid_gb.best_estimator_
print(f"Best params: {grid_gb.best_params_}")
evaluate_model("GRADIENT BOOSTING", y_test, best_gb.predict(X_test_scaled))

# 5. SVR
svr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)
svr.fit(X_train_scaled, y_train)
evaluate_model("SUPPORT VECTOR REGRESSION", y_test, svr.predict(X_test_scaled))

# 6. Stacking Ensemble
estimators = [('rf', best_rf), ('gb', best_gb), ('lr', lr)]
stacking = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())
stacking.fit(X_train_scaled, y_train)
evaluate_model("STACKING ENSEMBLE", y_test, stacking.predict(X_test_scaled))

# =============================================================================
# 12. LEARNING CURVES
# =============================================================================

print("\n" + "=" * 80)
print("LEARNING CURVES ANALYSIS")
print("=" * 80)

def analyze_learning_curve(estimator, X, y, title):
    """Génère et analyse la courbe d'apprentissage"""
    train_sizes, train_scores, val_scores = learning_curve(
        estimator, X, y, cv=5, scoring='neg_mean_absolute_error',
        train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1
    )
    
    train_mean = -train_scores.mean(axis=1)
    val_mean = -val_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    val_std = val_scores.std(axis=1)
    
    # Visualisation
    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_mean, 'o-', label='Training MAE', color='blue')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
    plt.plot(train_sizes, val_mean, 'o-', label='Validation MAE', color='red')
    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')
    plt.xlabel('Training Set Size')
    plt.ylabel('Mean Absolute Error')
    plt.title(f'Learning Curve - {title}')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig(f'learning_curve_{title.replace(" ", "_")}.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Analyse overfitting
    gap = val_mean[-1] - train_mean[-1]
    print(f"\n{title}:")
    print(f"  Training MAE: {train_mean[-1]:.2f} | Validation MAE: {val_mean[-1]:.2f}")
    print(f"  Gap: {gap:.2f}", end=" ")
    
    if gap > 1000:
        print("High overfitting!")
    elif gap > 500:
        print("Moderate overfitting")
    else:
        print("✓ Good generalization")

# Analyse des meilleurs modèles
for model, name in [(lr, "Linear Regression"), (best_rf, "Random Forest"), (best_gb, "Gradient Boosting")]:
    analyze_learning_curve(model, X_train_scaled, y_train, name)
